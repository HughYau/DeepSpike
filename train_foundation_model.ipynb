{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Applications\\anaconda3\\envs\\ag\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from dataset import spikeData\n",
    "from preprocess import preprocess, thresholdEvents\n",
    "from models import AE,VAE\n",
    "from clustering import *\n",
    "\n",
    "import argparse\n",
    "import pdb\n",
    "import random\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "def train(model,max_epoch=50):\n",
    "    #### Training and validation loop!\n",
    "    trLoss = []\n",
    "    trAcc = []\n",
    "    vlLoss = []\n",
    "    vlAcc = []\n",
    "    for epoch in range(max_epoch):          ### Run the model for max_epochs\n",
    "\n",
    "        epLoss = 0\n",
    "        for i,val  in enumerate(tqdm(train_loader)): \n",
    "            x,xIp,_ = val   ### Fetch a batch of training inputs\n",
    "            x, xIp = x.to(device), xIp.to(device)\n",
    "            xHat,z = model(x)               ####### Obtain a prediction from the network\n",
    "            loss = model.loss_function(xHat,xIp,z)     ######### Compute loss bw prediction and ground truth\n",
    "\n",
    "            ### Backpropagation steps\n",
    "            ### Clear old gradients\n",
    "            optimizer.zero_grad()\n",
    "            ### Compute the gradients wrt model weights\n",
    "            loss.backward()\n",
    "            ### Update the model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            epLoss += loss.item()\n",
    "\n",
    "        trLoss.append(epLoss/len(train_loader))\n",
    "\n",
    "        epLoss = 0\n",
    "        for x, xIp, _ in valid_loader: #### Fetch validation samples\n",
    "            x, xIp = x.to(device), xIp.to(device)\n",
    "            xHat,z = model(x) ##########\n",
    "            loss = model.loss_function(xHat,xIp,z) #######\n",
    "\n",
    "            epLoss += loss.item()\n",
    "        vlLoss.append(epLoss/len(valid_loader))\n",
    "\n",
    "        print('Epoch: %03d, Tr. Loss: %.4f, Vl.Loss: %.4f'\n",
    "              %(epoch,trLoss[-1],vlLoss[-1]))\n",
    "    plt.clf()\n",
    "    plt.plot(trLoss,label='training')\n",
    "    plt.plot(vlLoss,label='validation')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('loss_curve.pdf', dpi=300)\n",
    "    return model\n",
    "\n",
    "def train_VAE(model,max_epoch=50):\n",
    "    #### Training and validation loop!\n",
    "    trLoss = []\n",
    "    trAcc = []\n",
    "    vlLoss = []\n",
    "    vlAcc = []\n",
    "    \n",
    "     # scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, verbose=True)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-5, last_epoch=-1)\n",
    "    iters = len(train_loader)\n",
    "    for epoch in range(max_epoch):          ### Run the model for max_epochs\n",
    "\n",
    "        epLoss = 0\n",
    "        for i,val  in enumerate(tqdm(train_loader)): \n",
    "            x,xIp,_ = val   ### Fetch a batch of training inputs\n",
    "            x, xIp = x.to(device), xIp.to(device)\n",
    "            xHat, mean, log_var = model(x)               ####### Obtain a prediction from the network\n",
    "            loss = model.loss_function(xHat,xIp,mean,log_var)    ######### Compute loss bw prediction and ground truth\n",
    "\n",
    "            ### Backpropagation steps\n",
    "            ### Clear old gradients\n",
    "            optimizer.zero_grad()\n",
    "            ### Compute the gradients wrt model weights\n",
    "            loss.backward()\n",
    "            ### Update the model parameters\n",
    "            optimizer.step()\n",
    "            scheduler.step(epoch + i/iters)\n",
    "            epLoss += loss.item()\n",
    "            \n",
    "\n",
    "        trLoss.append(epLoss/len(train_loader))\n",
    "\n",
    "        epLoss = 0\n",
    "        for x, xIp, _ in valid_loader: #### Fetch validation samples\n",
    "            x, xIp = x.to(device), xIp.to(device)\n",
    "            xHat, mean, log_var = model(x)               ####### Obtain a prediction from the network\n",
    "            loss = model.loss_function(xHat,xIp,mean,log_var)    ######### Compute loss bw prediction and ground truth\n",
    "\n",
    "            epLoss += loss.item()\n",
    "        \n",
    "        val_loss = epLoss/len(valid_loader)\n",
    "        \n",
    "        \n",
    "        vlLoss.append(val_loss)\n",
    "\n",
    "        print('Epoch: %03d, Tr. Loss: %.6f, Vl.Loss: %.6f'\n",
    "              %(epoch,trLoss[-1],vlLoss[-1]))\n",
    "    plt.clf()\n",
    "    plt.plot(trLoss,label='training')\n",
    "    plt.plot(vlLoss,label='validation')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('loss_curve_VAEl1loss0529.pdf', dpi=300)\n",
    "    return model\n",
    "\n",
    "\n",
    "### Main starts here\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing E:\\Work\\SpikeSorting\\kiehnlab\\spikedate_pytorch\\foundation_data_all.pt\n"
     ]
    }
   ],
   "source": [
    "# data = 'foundation_data_all.pt'  # Spike data\n",
    "# output = None  # Output filename\n",
    "# thresh = 0.7  # Event detection threshold\n",
    "# cutoff = 0.5  # Base event detection threshold\n",
    "# poff = 12  # Positive offset after peak events\n",
    "# noff = 8  # Negative offset after peak events\n",
    "# corr = 0.0  # Correlation threshold for merging templates\n",
    "# mask = False  # Pretext task of masking input\n",
    "# kmeans = False  # Use kmeans for final clustering\n",
    "# jigsaw = False  # Pretext task of sorting jigsawed input\n",
    "# shuffle = False  # Pretext task of sorting shuffled input\n",
    "\n",
    "# epochs = 20  # Number of epochs\n",
    "# hidden = 2048  # Number of hidden units at input in AE\n",
    "# latent = 2  # Number of latent dimensions at bottleneck\n",
    "# batch = 1024  # Training batch size\n",
    "# lr = 0.0001  # Learning rate\n",
    "\n",
    "# ip_dim = 20  # Number of input features\n",
    "# save = True  # Save the trained model\n",
    "# denoise = False  # Denoise the data\n",
    "# reprocess = False  # Reprocess data\n",
    "# retrain = False  # Retrain model every threshold\n",
    "# model = None  # Pretrained model file\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.data = r\"E:\\Work\\SpikeSorting\\kiehnlab\\spikedate_pytorch\\foundation_data_all.pt\"\n",
    "        self.output = None\n",
    "        self.thresh = 0.7\n",
    "        self.cutoff = 0.5\n",
    "        self.poff = 12\n",
    "        self.noff = 8\n",
    "        self.corr = 0.0\n",
    "        self.mask = False\n",
    "        self.kmeans = False\n",
    "        self.jigsaw = False\n",
    "        self.shuffle = True\n",
    "        self.epochs = 50\n",
    "        self.hidden = 2048\n",
    "        self.latent = 8\n",
    "        self.batch = 4096\n",
    "        self.lr = 0.0002\n",
    "        self.ip_dim = 20\n",
    "        self.save = True\n",
    "        self.denoise = False\n",
    "        self.reprocess = False\n",
    "        self.retrain = False\n",
    "        self.model = None\n",
    "\n",
    "args = Args()\n",
    "print(f'Loading and preprocessing {args.data.split(\"/\")[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model as models/model_H_2048L_8_VAEMSE_0524_shuffle\n",
      "Found 4826366 events at 0.7000 threshold\n",
      "################## Using Threshold=0.70 ##############\n",
      "Using shuffle pretext\n",
      "Found 4826366 events at 0.70 threshold\n",
      "Ntrain: 3861092, NValid: 965274\n",
      "Training on every datafile...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [02:32<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Tr. Loss: 0.080343, Vl.Loss: 0.049042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [02:31<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Tr. Loss: 0.051211, Vl.Loss: 0.049063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [02:33<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Tr. Loss: 0.050866, Vl.Loss: 0.048989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [03:45<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Tr. Loss: 0.050752, Vl.Loss: 0.049004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [02:18<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Tr. Loss: 0.050694, Vl.Loss: 0.048982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [02:26<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Tr. Loss: 0.050564, Vl.Loss: 0.048965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [02:22<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Tr. Loss: 0.050516, Vl.Loss: 0.048906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 943/943 [03:39<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Tr. Loss: 0.050465, Vl.Loss: 0.048907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 196/943 [00:34<02:11,  5.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 64\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining on every datafile...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 64\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_VAE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m### Save the trained model\u001b[39;00m\n\u001b[0;32m     67\u001b[0m mName \u001b[38;5;241m=\u001b[39m model_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[1], line 86\u001b[0m, in \u001b[0;36mtrain_VAE\u001b[1;34m(model, max_epoch)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epoch):          \u001b[38;5;66;03m### Run the model for max_epochs\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     epLoss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i,val  \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader)): \n\u001b[0;32m     87\u001b[0m         x,xIp,_ \u001b[38;5;241m=\u001b[39m val   \u001b[38;5;66;03m### Fetch a batch of training inputs\u001b[39;00m\n\u001b[0;32m     88\u001b[0m         x, xIp \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), xIp\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\ag\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\ag\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\ag\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\ag\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\ag\\lib\\site-packages\\torch\\utils\\data\\dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\ag\\lib\\site-packages\\torch\\utils\\data\\dataset.py:399\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32me:\\Work\\SpikeSorting\\deepspike-master\\dataset.py:80\u001b[0m, in \u001b[0;36mspikeData.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshuffle:\n\u001b[0;32m     79\u001b[0m     idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH)\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_index[index]\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjigsaw:\n\u001b[0;32m     82\u001b[0m     idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_name = args.model\n",
    "if model_name == None:\n",
    "    model_name = 'models/model_H_'+repr(args.hidden)+'L_'+repr(args.latent)+'_VAEMSE_0524' \n",
    "if args.shuffle:\n",
    "    model_name += '_shuffle'\n",
    "elif args.mask:\n",
    "    model_name += '_mask'\n",
    "elif args.jigsaw:\n",
    "    model_name += '_jigsaw'\n",
    "print('Saving model as '+model_name)\n",
    "\n",
    "### Obtain a distribution of number of events for different thresholds\n",
    "num_events = []\n",
    "thresh = args.thresh\n",
    "\n",
    "### Instantiate a model! \n",
    "nIp = args.ip_dim \n",
    "model = VAE(nIp=nIp,nhid=args.hidden,latent_dim=args.latent) \n",
    "model = model.to(device)\n",
    "criterion = nn.L1Loss() ############ Loss function to be optimized. \n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=args.lr) \n",
    "#pdb.set_trace()\n",
    "### Load pretrained model\n",
    "\n",
    "if args.retrain:\n",
    "### Train the neural network!\n",
    "    print(\"Training on every datafile...\")\n",
    "elif (args.model is not None):\n",
    "    model.load_state_dict(torch.load(args.model,map_location=device))\n",
    "    print(\"Using pretrained model...\")\n",
    "    model_name = args.model[:-9]\n",
    "\n",
    "events = (torch.load(args.data)).squeeze()\n",
    "events = events[[(events.max(-1)[0] <= 1.0)]].unsqueeze(1).unsqueeze(1)\n",
    "posEventIdx = thresholdEvents(events,thresh)\n",
    "N = len(events[posEventIdx])\n",
    "print(\"Found %d events at %.4f threshold\"%(len(events[posEventIdx]),thresh))\n",
    "\n",
    "tmp = 0\n",
    "    \n",
    "print(\"################## Using Threshold=%.2f ##############\"%thresh)\n",
    "### Make torch dataset\n",
    "#### Make training, validation and test sets\n",
    "evClsLabel = np.zeros(len(events),dtype=int)\n",
    "eventSeq = np.arange(len(events),dtype=int)\n",
    "dataset = spikeData(data=events, evMask=evClsLabel,mask=args.mask,event_index=eventSeq,\\\n",
    "    jigsaw=args.jigsaw,shuffle=args.shuffle,thresh=thresh)\n",
    "\n",
    "\n",
    "nTrain = int(0.8*N)\n",
    "nValid = N - nTrain\n",
    "train_set = Subset(dataset, list(range(nTrain))) \n",
    "valid_set = Subset(dataset, list(range(nTrain,N))) #random_split(dataset,[nTrain, nValid])\n",
    "B = args.batch\n",
    "\n",
    "### Wrapping the datasets with DataLoader class \n",
    "train_loader = DataLoader(train_set,batch_size=B, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set,batch_size=B, shuffle=False)\n",
    "print(\"Ntrain: %d, NValid: %d\"%(nTrain,nValid))\n",
    "\n",
    "### Train the neural network!\n",
    "print(\"Training on every datafile...\")\n",
    "model = model.to(device)\n",
    "model = train_VAE(model,args.epochs)\n",
    "    \n",
    "### Save the trained model\n",
    "mName = model_name+'.pt'\n",
    "print('Saving model '+mName)\n",
    "torch.save(model.state_dict(), mName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
